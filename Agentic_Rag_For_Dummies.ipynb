{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GiovanniPasq/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "4719bde215637659"
  },
  {
   "cell_type": "code",
   "id": "321dbfd3",
   "metadata": {
    "id": "321dbfd3",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:23:50.260184Z",
     "start_time": "2025-11-26T11:23:24.500717Z"
    }
   },
   "source": [
    "#run this cell only in colab, otherwise create a venv and install requirements.txt available in the project folder\n",
    "!pip install --quiet --upgrade langgraph\n",
    "!pip install -qU langchain-ollama\n",
    "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
    "!pip install --upgrade gradio\n",
    "\n",
    "# Optional (example): if you want to use Gemini models\n",
    "#!pip install -qU \"langchain[google-genai]\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (6.0.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-6.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.121.3)\n",
      "Requirement already satisfied: ffmpy in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Collecting gradio-client==2.0.0 (from gradio)\n",
      "  Downloading gradio_client-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: groovy~=0.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (2.3.5)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (2.12.4)\n",
      "Requirement already satisfied: pydub in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from gradio-client==2.0.0->gradio) (2025.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: requests in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\agentic-rag-for-dummies\\.venv\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
      "Downloading gradio-6.0.1-py3-none-any.whl (21.6 MB)\n",
      "   ---------------------------------------- 0.0/21.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/21.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/21.6 MB 8.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 4.5/21.6 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.8/21.6 MB 9.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 8.9/21.6 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 10.2/21.6 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 13.1/21.6 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 15.2/21.6 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 17.0/21.6 MB 9.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 19.1/21.6 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 21.0/21.6 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 21.6/21.6 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading gradio_client-2.0.0-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: gradio-client, gradio\n",
      "\n",
      "  Attempting uninstall: gradio-client\n",
      "\n",
      "    Found existing installation: gradio_client 2.0.0.dev3\n",
      "\n",
      "    Uninstalling gradio_client-2.0.0.dev3:\n",
      "\n",
      "      Successfully uninstalled gradio_client-2.0.0.dev3\n",
      "\n",
      "  Attempting uninstall: gradio\n",
      "\n",
      "    Found existing installation: gradio 6.0.0\n",
      "\n",
      "    Uninstalling gradio-6.0.0:\n",
      "\n",
      "      Successfully uninstalled gradio-6.0.0\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   -------------------- ------------------- 1/2 [gradio]\n",
      "   ---------------------------------------- 2/2 [gradio]\n",
      "\n",
      "Successfully installed gradio-6.0.1 gradio-client-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c9782958",
   "metadata": {
    "id": "c9782958",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:26:55.963867Z",
     "start_time": "2025-11-26T11:26:55.958743Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: Configuration and Environment Setup\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "DOCS_DIR = \"project/docs\"  # Directory containing your pdfs files\n",
    "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
    "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
    "CHILD_COLLECTION = \"document_child_chunks\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
    "os.makedirs(PARENT_STORE_PATH, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7e8989b7",
   "metadata": {
    "id": "7e8989b7",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:28:31.000461Z",
     "start_time": "2025-11-26T11:28:27.451Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: LLM Initialization\n",
    "# ============================================================\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"gpt-oss:20b\", temperature=0)\n",
    "\n",
    "# Alternative (example): Google Gemini\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\agentic-rag-for-dummies\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9f982c53",
   "metadata": {
    "id": "9f982c53",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:30:02.631505Z",
     "start_time": "2025-11-26T11:29:07.784416Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 3: Embeddings Setup\n",
    "# ============================================================\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "\n",
    "# Dense embeddings for semantic understanding\n",
    "dense_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Sparse embeddings for keyword matching\n",
    "sparse_embeddings = FastEmbedSparse(\n",
    "    model_name=\"Qdrant/bm25\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\agentic-rag-for-dummies\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dimlock\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]D:\\Projects\\agentic-rag-for-dummies\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dimlock\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bm25. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:02<00:00,  7.45it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "11efc63b",
   "metadata": {
    "id": "11efc63b",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:30:22.725641Z",
     "start_time": "2025-11-26T11:30:22.624988Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 4: Vector Database Setup\n",
    "# ============================================================\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant.qdrant import RetrievalMode\n",
    "\n",
    "# Initialize Qdrant client (local file-based storage)\n",
    "client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "\n",
    "def ensure_collection(collection_name):\n",
    "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=embedding_dimension,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": qmodels.SparseVectorParams()\n",
    "            },\n",
    "        )\n",
    "        print(f\"‚úì Created collection: {collection_name}\")\n",
    "    else:\n",
    "        print(f\"‚úì Collection already exists: {collection_name}\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "06fd6518",
   "metadata": {
    "id": "06fd6518",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:44:27.518358Z",
     "start_time": "2025-11-26T11:44:21.621308Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 5: PDFs Conversion to Markdown\n",
    "# ============================================================\n",
    "\n",
    "# For more details on converting PDFs to Markdown, refer to the `pdf_to_md.ipynb` notebook available in the repository.\n",
    "import os\n",
    "import pymupdf.layout\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_dir):\n",
    "\n",
    "\n",
    "    source = pdf_path  # file path or URL\n",
    "    converter = DocumentConverter()\n",
    "    doc = converter.convert(source).document\n",
    "    md = doc.export_to_markdown()\n",
    "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
    "    output_path = Path(output_dir) / Path(doc.name).stem\n",
    "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
    "\n",
    "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
    "    output_dir = Path(MARKDOWN_DIR)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
    "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
    "        if overwrite or not md_path.exists():\n",
    "            pdf_to_markdown(pdf_path, output_dir)\n",
    "\n",
    "pdfs_to_markdowns(\"project/docs/*.docx\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 14:44:24,134 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:24,145 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:24,145 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:24,155 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-26 14:44:24,155 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-26 14:44:24,155 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Å–∏–º –∫–∞—Ä—Ç.docx\n",
      "2025-11-26 14:44:24,200 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Å–∏–º –∫–∞—Ä—Ç.docx in 0.06 sec.\n",
      "2025-11-26 14:44:24,226 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:24,247 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:24,248 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:24,249 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—ã–ø—É—Å–∫ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ VPN.docx\n",
      "2025-11-26 14:44:24,455 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—ã–ø—É—Å–∫ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ VPN.docx in 0.23 sec.\n",
      "2025-11-26 14:44:24,466 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:24,471 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:24,471 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:24,471 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤.docx\n",
      "2025-11-26 14:44:24,857 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤.docx in 0.39 sec.\n",
      "2025-11-26 14:44:24,867 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:24,878 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:24,878 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:24,878 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –≤—Ö–æ–¥–∏—Ç—å –Ω–∞ —É–¥–∞–ª–µ–Ω–∫—É.docx\n",
      "2025-11-26 14:44:25,011 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –≤—Ö–æ–¥–∏—Ç—å –Ω–∞ —É–¥–∞–ª–µ–Ω–∫—É.docx in 0.14 sec.\n",
      "2025-11-26 14:44:25,021 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,021 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,021 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,021 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –∑–∞–≤–µ—Å—Ç–∏ –≤ –¥–æ–º–µ–Ω.docx\n",
      "2025-11-26 14:44:25,073 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –∑–∞–≤–µ—Å—Ç–∏ –≤ –¥–æ–º–µ–Ω.docx in 0.05 sec.\n",
      "2025-11-26 14:44:25,083 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,083 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,083 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,088 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–æ–≤—ã–π –ø–ª–∞–Ω—à–µ—Ç —Å –Ω—É–ª—è.docx\n",
      "2025-11-26 14:44:25,104 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–æ–≤—ã–π –ø–ª–∞–Ω—à–µ—Ç —Å –Ω—É–ª—è.docx in 0.02 sec.\n",
      "2025-11-26 14:44:25,115 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,119 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,119 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,119 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç.docx\n",
      "2025-11-26 14:44:25,259 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç.docx in 0.14 sec.\n",
      "2025-11-26 14:44:25,269 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,292 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,293 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,293 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VPN.docx\n",
      "2025-11-26 14:44:25,602 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VPN.docx in 0.33 sec.\n",
      "2025-11-26 14:44:25,615 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,627 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,627 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,628 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ VPN.docx\n",
      "2025-11-26 14:44:25,753 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ VPN.docx in 0.14 sec.\n",
      "2025-11-26 14:44:25,763 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:25,776 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:25,777 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:25,777 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–∫–∏ VPN.docx\n",
      "2025-11-26 14:44:26,090 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–∫–∏ VPN.docx in 0.33 sec.\n",
      "2025-11-26 14:44:26,102 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:26,111 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:26,111 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:26,112 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ bat –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ 1–°.docx\n",
      "2025-11-26 14:44:26,754 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ bat –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ 1–°.docx in 0.66 sec.\n",
      "2025-11-26 14:44:26,773 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:26,778 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:26,778 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:26,779 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.docx\n",
      "2025-11-26 14:44:26,918 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.docx in 0.14 sec.\n",
      "2025-11-26 14:44:26,927 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:26,930 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:26,930 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:26,930 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é —Ä–∞–∑–Ω–æ—Å—Ç–Ω–æ–≥–æ –ë—ç–∫–∞–ø–∞ SQL.docx\n",
      "2025-11-26 14:44:26,930 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é —Ä–∞–∑–Ω–æ—Å—Ç–Ω–æ–≥–æ –ë—ç–∫–∞–ø–∞ SQL.docx in 0.00 sec.\n",
      "2025-11-26 14:44:26,944 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:26,947 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:26,948 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:26,949 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∑–∞–ø–∞—Å–Ω–æ–≥–æ –í–ü–ù.docx\n",
      "2025-11-26 14:44:26,983 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∑–∞–ø–∞—Å–Ω–æ–≥–æ –í–ü–ù.docx in 0.03 sec.\n",
      "2025-11-26 14:44:26,991 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:26,998 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:26,999 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,000 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –¢–°–î.docx\n",
      "2025-11-26 14:44:27,116 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –¢–°–î.docx in 0.12 sec.\n",
      "2025-11-26 14:44:27,124 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,128 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,129 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,130 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—á–∞—Ç–∏ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞..docx\n",
      "2025-11-26 14:44:27,192 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—á–∞—Ç–∏ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞..docx in 0.08 sec.\n",
      "2025-11-26 14:44:27,198 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,200 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,201 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,201 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö.docx\n",
      "2025-11-26 14:44:27,212 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö.docx in 0.02 sec.\n",
      "2025-11-26 14:44:27,219 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,222 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,222 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,223 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä –ú–æ–Ω–æ–ª–∏—Ç.docx\n",
      "2025-11-26 14:44:27,227 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä –ú–æ–Ω–æ–ª–∏—Ç.docx in 0.02 sec.\n",
      "2025-11-26 14:44:27,240 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,244 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,244 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,245 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä—ã.docx\n",
      "2025-11-26 14:44:27,317 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä—ã.docx in 0.08 sec.\n",
      "2025-11-26 14:44:27,337 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,342 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,342 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,342 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –°–∏–º-–∫–∞—Ä—Ç–∞ –∫–æ–æ–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è.docx\n",
      "2025-11-26 14:44:27,389 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –°–∏–º-–∫–∞—Ä—Ç–∞ –∫–æ–æ–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è.docx in 0.05 sec.\n",
      "2025-11-26 14:44:27,399 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,399 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,399 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,399 - INFO - Processing document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¢–°–î –ú–∞—Ö–∞—á–∫–∞–ª–∞.docx\n",
      "2025-11-26 14:44:27,420 - INFO - Finished converting document –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¢–°–î –ú–∞—Ö–∞—á–∫–∞–ª–∞.docx in 0.02 sec.\n",
      "2025-11-26 14:44:27,432 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,432 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,438 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,438 - INFO - Processing document –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ_–∫–ª–∏–µ–Ω—Ç–∞_Citrix_Workspace_—á–µ—Ä–µ–∑_VPN_–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è.docx\n",
      "2025-11-26 14:44:27,461 - INFO - Finished converting document –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ_–∫–ª–∏–µ–Ω—Ç–∞_Citrix_Workspace_—á–µ—Ä–µ–∑_VPN_–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è.docx in 0.03 sec.\n",
      "2025-11-26 14:44:27,474 - INFO - detected formats: [<InputFormat.DOCX: 'docx'>]\n",
      "2025-11-26 14:44:27,474 - INFO - Going to convert document batch...\n",
      "2025-11-26 14:44:27,481 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-11-26 14:44:27,481 - INFO - Processing document –®–∞–±–ª–æ–Ω –æ–ø–∏—Å–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–∞.docx\n",
      "2025-11-26 14:44:27,502 - INFO - Finished converting document –®–∞–±–ª–æ–Ω –æ–ø–∏—Å–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–∞.docx in 0.03 sec.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e103d352",
   "metadata": {
    "id": "e103d352",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:45:36.927773Z",
     "start_time": "2025-11-26T11:45:16.825389Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 6: Document Indexing\n",
    "# ============================================================\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "if client.collection_exists(CHILD_COLLECTION):\n",
    "    print(f\"Removing existing Qdrant collection: {CHILD_COLLECTION}\")\n",
    "    client.delete_collection(CHILD_COLLECTION)\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "else:\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "\n",
    "child_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=CHILD_COLLECTION,\n",
    "    embedding=dense_embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    sparse_vector_name=\"sparse\"\n",
    ")\n",
    "\n",
    "def index_documents():\n",
    "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "    parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    min_parent_size = 2000\n",
    "    max_parent_size = 10000\n",
    "\n",
    "    all_parent_pairs, all_child_chunks = [], []\n",
    "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
    "\n",
    "    if not md_files:\n",
    "        print(f\"‚ö†Ô∏è  No .md files found in {MARKDOWN_DIR}/\")\n",
    "        return\n",
    "\n",
    "    for doc_path_str in md_files:\n",
    "        doc_path = Path(doc_path_str)\n",
    "        print(f\"üìÑ Processing: {doc_path.name}\")\n",
    "\n",
    "        try:\n",
    "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                md_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {doc_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        parent_chunks = parent_splitter.split_text(md_text)\n",
    "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
    "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
    "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size)\n",
    "\n",
    "        for i, p_chunk in enumerate(cleaned_parents):\n",
    "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
    "            p_chunk.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
    "            all_parent_pairs.append((parent_id, p_chunk))\n",
    "            children = child_splitter.split_documents([p_chunk])\n",
    "            all_child_chunks.extend(children)\n",
    "\n",
    "    if not all_child_chunks:\n",
    "        print(\"‚ö†Ô∏è No child chunks to index\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüîç Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
    "    try:\n",
    "        child_vector_store.add_documents(all_child_chunks)\n",
    "        print(\"‚úì Child chunks indexed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error indexing child chunks: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üíæ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
    "    for item in os.listdir(PARENT_STORE_PATH):\n",
    "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
    "\n",
    "    for parent_id, doc in all_parent_pairs:\n",
    "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def merge_small_parents(chunks, min_size):\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged, current = [], None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if current is None:\n",
    "            current = chunk\n",
    "        else:\n",
    "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
    "            for k, v in chunk.metadata.items():\n",
    "                if k in current.metadata:\n",
    "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    current.metadata[k] = v\n",
    "\n",
    "        if len(current.page_content) >= min_size:\n",
    "            merged.append(current)\n",
    "            current = None\n",
    "\n",
    "    if current:\n",
    "        if merged:\n",
    "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
    "            for k, v in current.metadata.items():\n",
    "                if k in merged[-1].metadata:\n",
    "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    merged[-1].metadata[k] = v\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def split_large_parents(chunks, max_size, splitter):\n",
    "    split_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) <= max_size:\n",
    "            split_chunks.append(chunk)\n",
    "        else:\n",
    "            large_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=max_size,\n",
    "                chunk_overlap=splitter._chunk_overlap\n",
    "            )\n",
    "            sub_chunks = large_splitter.split_documents([chunk])\n",
    "            split_chunks.extend(sub_chunks)\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "def clean_small_chunks(chunks, min_size):\n",
    "    cleaned = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk.page_content) < min_size:\n",
    "            if cleaned:\n",
    "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in cleaned[-1].metadata:\n",
    "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
    "                    else:\n",
    "                        cleaned[-1].metadata[k] = v\n",
    "            elif i < len(chunks) - 1:\n",
    "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in chunks[i + 1].metadata:\n",
    "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
    "                    else:\n",
    "                        chunks[i + 1].metadata[k] = v\n",
    "            else:\n",
    "                cleaned.append(chunk)\n",
    "        else:\n",
    "            cleaned.append(chunk)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "index_documents()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created collection: document_child_chunks\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –°–∏–º-–∫–∞—Ä—Ç–∞ –∫–æ–æ–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¢–°–î –ú–∞—Ö–∞—á–∫–∞–ª–∞.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Å–∏–º –∫–∞—Ä—Ç.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—ã–ø—É—Å–∫ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ VPN.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –≤—Ö–æ–¥–∏—Ç—å –Ω–∞ —É–¥–∞–ª–µ–Ω–∫—É.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –∑–∞–≤–µ—Å—Ç–∏ –≤ –¥–æ–º–µ–Ω.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–æ–≤—ã–π –ø–ª–∞–Ω—à–µ—Ç —Å –Ω—É–ª—è.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VPN.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ VPN.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–∫–∏ VPN.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ bat –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ 1–°.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é —Ä–∞–∑–Ω–æ—Å—Ç–Ω–æ–≥–æ –ë—ç–∫–∞–ø–∞ SQL.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –¢–°–î.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∑–∞–ø–∞—Å–Ω–æ–≥–æ –í–ü–ù.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—á–∞—Ç–∏ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞..md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä –ú–æ–Ω–æ–ª–∏—Ç.md\n",
      "üìÑ Processing: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–∏–Ω—Ç–µ—Ä—ã.md\n",
      "üìÑ Processing: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ_–∫–ª–∏–µ–Ω—Ç–∞_Citrix_Workspace_—á–µ—Ä–µ–∑_VPN_–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è.md\n",
      "üìÑ Processing: –®–∞–±–ª–æ–Ω –æ–ø–∏—Å–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–∞.md\n",
      "\n",
      "üîç Indexing 90 child chunks into Qdrant...\n",
      "‚úì Child chunks indexed successfully\n",
      "üíæ Saving 23 parent chunks to JSON...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "b8eac029",
   "metadata": {
    "id": "b8eac029",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:45:56.809671Z",
     "start_time": "2025-11-26T11:45:56.782380Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 7: Tool Definitions\n",
    "# ============================================================\n",
    "import json\n",
    "from typing import List\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_child_chunks(query: str, k: int = 5) -> List[dict]:\n",
    "    \"\"\"Search for the top K most relevant child chunks.\n",
    "\n",
    "    Args:\n",
    "        query: Search query string\n",
    "        k: Number of results to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = child_vector_store.similarity_search(query, k=k, score_threshold=0.7)\n",
    "        return [\n",
    "            {\n",
    "                \"content\": doc.page_content,\n",
    "                \"parent_id\": doc.metadata.get(\"parent_id\", \"\"),\n",
    "                \"source\": doc.metadata.get(\"source\", \"\")\n",
    "            }\n",
    "            for doc in results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching child chunks: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def retrieve_parent_chunks(parent_ids: List[str]) -> List[dict]:\n",
    "    \"\"\"Retrieve full parent chunks by their IDs.\n",
    "\n",
    "    Args:\n",
    "        parent_ids: List of parent chunk IDs to retrieve\n",
    "    \"\"\"\n",
    "    unique_ids = sorted(list(set(parent_ids)))\n",
    "    results = []\n",
    "\n",
    "    for parent_id in unique_ids:\n",
    "        file_path = os.path.join(PARENT_STORE_PATH, parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    doc_dict = json.load(f)\n",
    "                    results.append({\n",
    "                        \"content\": doc_dict[\"page_content\"],\n",
    "                        \"parent_id\": parent_id,\n",
    "                        \"metadata\": doc_dict[\"metadata\"]\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading parent chunk {parent_id}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b12809c6",
   "metadata": {
    "id": "b12809c6",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:46:04.856551Z",
     "start_time": "2025-11-26T11:46:04.852179Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 8: Agent System Prompt\n",
    "# ============================================================\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an intelligent assistant that MUST use the available tools to answer questions.\n",
    "\n",
    "**MANDATORY WORKFLOW ‚Äî Follow these steps for EVERY question:**\n",
    "\n",
    "1. **Call `search_child_chunks`** with the user's query (K = 3‚Äì7).\n",
    "\n",
    "2. **Review the retrieved chunks** and identify the relevant ones.\n",
    "\n",
    "3. **For each relevant chunk, call `retrieve_parent_chunks`** using its parent_id to get full context.\n",
    "\n",
    "4. **If the retrieved context is still incomplete, retrieve additional parent chunks** as needed.\n",
    "\n",
    "5. **If metadata helps clarify or support the answer, USE IT**\n",
    "\n",
    "6. **Answer using ONLY the retrieved information**\n",
    "   - Cite source files from metadata.\n",
    "\n",
    "7. **If no relevant information is found,** rewrite the query into an **answer-focused declarative statement** and search again **only once**.\n",
    "\"\"\"\n",
    "\n",
    "agent_system_message = SystemMessage(content=AGENT_SYSTEM_PROMPT)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c4c066b1",
   "metadata": {
    "id": "c4c066b1",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:46:11.799977Z",
     "start_time": "2025-11-26T11:46:11.729614Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 9: State Definitions\n",
    "# ============================================================\n",
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class State(MessagesState):\n",
    "    questionIsClear: bool\n",
    "    conversation_summary: str = \"\"\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    is_clear: bool = Field(\n",
    "        description=\"Indicates if the user's question is clear and answerable.\"\n",
    "    )\n",
    "    questions: List[str] = Field(\n",
    "        description=\"List of rewritten, self-contained questions.\"\n",
    "    )\n",
    "    clarification_needed: str = Field(\n",
    "        description=\"Explanation if the question is unclear.\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "948ca8ee",
   "metadata": {
    "id": "948ca8ee",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:46:25.017782Z",
     "start_time": "2025-11-26T11:46:25.005850Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 10: Graph Node Functions\n",
    "# ============================================================\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    "from typing import Literal\n",
    "\n",
    "def analyze_chat_and_summarize(state: State):\n",
    "    \"\"\"\n",
    "    Analyzes chat history and summarizes key points for context.\n",
    "    \"\"\"\n",
    "    if len(state[\"messages\"]) < 4:  # Need some history to summarize\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "\n",
    "    # Extract relevant messages (excluding current query and system messages)\n",
    "    relevant_msgs = [\n",
    "        msg for msg in state[\"messages\"][:-1]  # Exclude current query\n",
    "        if isinstance(msg, (HumanMessage, AIMessage))\n",
    "        and not getattr(msg, \"tool_calls\", None)\n",
    "    ]\n",
    "\n",
    "    if not relevant_msgs:\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "\n",
    "    summary_prompt = \"\"\"**Summarize the key topics and context from this conversation concisely (1-2 sentences max).**\n",
    "    Discard irrelevant information, such as misunderstandings or off-topic queries/responses.\n",
    "    If there are no key topics, return an empty string.\n",
    "\n",
    "    \"\"\"\n",
    "    for msg in relevant_msgs[-6:]:  # Last 6 messages for context\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        summary_prompt += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nBrief Summary:\"\n",
    "    summary_response = llm.with_config(temperature=0.3).invoke([SystemMessage(content=summary_prompt)])\n",
    "    return {\"conversation_summary\": summary_response.content}\n",
    "\n",
    "def analyze_and_rewrite_query(state: State):\n",
    "    \"\"\"\n",
    "    Analyzes user query and rewrites it for clarity, optionally using conversation context.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
    "\n",
    "    context_section = (\n",
    "        f\"**Conversation Context:**\\n{conversation_summary}\"\n",
    "        if conversation_summary.strip()\n",
    "        else \"**Conversation Context:**\\n[First query in conversation]\"\n",
    "    )\n",
    "\n",
    "    # Create analysis prompt\n",
    "    prompt = f\"\"\"\n",
    "    **Rewrite the user's query** to be clear, self-contained, and optimized for information retrieval.\n",
    "\n",
    "    **User Query:**\n",
    "    \"{last_message.content}\"\n",
    "\n",
    "    {context_section}\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1. **Resolve references for follow-ups:**\n",
    "    - If the query uses pronouns or refers to previous topics, use the context to make it self-contained.\n",
    "\n",
    "    2. **Ensure clarity for new queries:**\n",
    "    - Make the query specific, concise, and unambiguous.\n",
    "\n",
    "    3. **Correct errors and interpret intent:**\n",
    "    - If the query is grammatically incorrect, contains typos, or has abbreviations, correct it and infer the intended meaning.\n",
    "\n",
    "    4. **Split only when necessary:**\n",
    "    - If multiple distinct questions exist, split into **up to 3 focused sub-queries** to avoid over-segmentation.\n",
    "    - Each sub-query must still be meaningful on its own.\n",
    "\n",
    "    5. **Optimize for search:**\n",
    "    - Use **keywords, proper nouns, numbers, dates, and technical terms**.\n",
    "    - Remove conversational filler, vague words, and redundancies.\n",
    "    - Make the query concise and focused for information retrieval.\n",
    "\n",
    "    6. **Mark as unclear if intent is missing:**\n",
    "    - This includes nonsense, gibberish, insults, or statements without an apparent question.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_with_structure = llm.with_config(temperature=0.3).with_structured_output(QueryAnalysis)\n",
    "    response = llm_with_structure.invoke([SystemMessage(content=prompt)])\n",
    "\n",
    "    if response.is_clear:\n",
    "        # Remove all non-system messages\n",
    "        delete_all = [\n",
    "            RemoveMessage(id=m.id)\n",
    "            for m in state[\"messages\"]\n",
    "            if not isinstance(m, SystemMessage)\n",
    "        ]\n",
    "\n",
    "        # Format rewritten query\n",
    "        rewritten = (\n",
    "            \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(response.questions)])\n",
    "            if len(response.questions) > 1\n",
    "            else response.questions[0]\n",
    "        )\n",
    "        return {\n",
    "            \"questionIsClear\": True,\n",
    "            \"messages\": delete_all + [HumanMessage(content=rewritten)]\n",
    "        }\n",
    "    else:\n",
    "        clarification = response.clarification_needed or \"I need more information to understand your question.\"\n",
    "        return {\n",
    "            \"questionIsClear\": False,\n",
    "            \"messages\": [AIMessage(content=clarification)]\n",
    "        }\n",
    "\n",
    "def human_input_node(state: State):\n",
    "    \"\"\"Placeholder node for human-in-the-loop interruption\"\"\"\n",
    "    return {}\n",
    "\n",
    "def route_after_rewrite(state: State) -> Literal[\"agent\", \"human_input\"]:\n",
    "    \"\"\"Route to agent if question is clear, otherwise wait for human input\"\"\"\n",
    "    return \"agent\" if state.get(\"questionIsClear\", False) else \"human_input\"\n",
    "\n",
    "def agent_node(state: State):\n",
    "    \"\"\"Main agent node that processes queries using tools\"\"\"\n",
    "    messages = [SystemMessage(content=agent_system_message.content)] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d8c111af",
   "metadata": {
    "id": "d8c111af",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:46:29.315970Z",
     "start_time": "2025-11-26T11:46:29.293389Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 11: Graph Construction\n",
    "# ============================================================\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Initialize checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create graph builder\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"summarize\", analyze_chat_and_summarize)\n",
    "graph_builder.add_node(\"analyze_rewrite\", analyze_and_rewrite_query)\n",
    "graph_builder.add_node(\"human_input\", human_input_node)\n",
    "graph_builder.add_node(\"agent\", agent_node)\n",
    "graph_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"summarize\")\n",
    "graph_builder.add_edge(\"summarize\", \"analyze_rewrite\")\n",
    "graph_builder.add_conditional_edges(\"analyze_rewrite\", route_after_rewrite)\n",
    "graph_builder.add_edge(\"human_input\", \"analyze_rewrite\")\n",
    "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile graph\n",
    "agent_graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human_input\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì Agent graph compiled successfully.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent graph compiled successfully.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "5aadf1ed",
   "metadata": {
    "id": "5aadf1ed",
    "ExecuteTime": {
     "end_time": "2025-11-26T11:52:46.242135Z",
     "start_time": "2025-11-26T11:52:45.307412Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# BLOCK 12: Gradio Interface\n",
    "# ============================================================\n",
    "\n",
    "#For a complete end-to-end pipeline Gradio interface, including document ingestion, please refer to the project folder\n",
    "\n",
    "import gradio as gr\n",
    "import uuid\n",
    "\n",
    "def create_thread_id():\n",
    "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
    "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "def clear_session():\n",
    "    \"\"\"Clear thread for new conversation and clean up checkpointer state\"\"\"\n",
    "    global config\n",
    "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
    "    config = create_thread_id()\n",
    "\n",
    "def chat_with_agent(message, history):\n",
    "    \"\"\"\n",
    "    Handle chat with human-in-the-loop support.\n",
    "    Returns: response text\n",
    "    \"\"\"\n",
    "    current_state = agent_graph.get_state(config)\n",
    "    if current_state.next:\n",
    "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
    "        result = agent_graph.invoke(None, config)\n",
    "    else:\n",
    "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "# Initialize thread configuration\n",
    "config = create_thread_id()\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=600, placeholder=\"<strong>Ask me anything!</strong><br><em>I'll search, reason, and act to give you the best answer :)</em>\")\n",
    "    chatbot.clear(clear_session)\n",
    "    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n",
    "\n",
    "print(\"\\nLaunching application...\")\n",
    "demo.launch(theme=gr.themes.Citrus())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launching application...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 14:52:46,160 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 14:52:46,203 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 14:52:46,593 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-11-26 14:52:46,682 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-11-26 14:53:06,680 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bba8a6e318ef329d"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
